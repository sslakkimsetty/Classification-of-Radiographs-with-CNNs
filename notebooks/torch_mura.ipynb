{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_mura.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "17Jw8OdFXek5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xy4veg9HpHlS",
        "colab_type": "code",
        "outputId": "b16027e3-2bca-4934-c614-8e2157508419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jvhLCOwpVUEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/MURA_files/data/MURA-v1.1.zip MURA-v1.1.zip\n",
        "!unzip -q MURA-v1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K4tovZGmW2Ry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/MURA_files/data/processed processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rtkuz4jPW8iX",
        "colab_type": "code",
        "outputId": "61f60200-7f8c-4218-a2ab-b69c6ff9f0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/processed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_all.csv\t   train_hand.csv      valid_all.csv\t  valid_hand.csv\n",
            "train_elbow.csv    train_humerus.csv   valid_elbow.csv\t  valid_humerus.csv\n",
            "train_finger.csv   train_shoulder.csv  valid_finger.csv   valid_shoulder.csv\n",
            "train_forearm.csv  train_wrist.csv     valid_forearm.csv  valid_wrist.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lXYr-pQcYQPa",
        "colab_type": "code",
        "outputId": "31a5a3be-f9eb-4b1b-b1f7-949ef0fcaeed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "1qaUPTsCePnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mura_prefix = '/content/'\n",
        "train_all = '/content/processed/train_all.csv'\n",
        "valid_all = '/content/processed/valid_all.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INIbdyn2kdDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MuraDataset(data.Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, columns=[3, 4]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "            columns (int array, optional): an array of length 2. first element\n",
        "                is the column that specifies path in the csv and second is\n",
        "                the label for that path\n",
        "        \"\"\"\n",
        "        self.process_csv(csv_file, columns)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "    def process_csv(self, file, columns):\n",
        "        # read and update\n",
        "        df = pd.read_csv(file)\n",
        "        df = df.iloc[:, columns]\n",
        "        df.columns = ['path', 'label']\n",
        "        \n",
        "        # encode labels\n",
        "        le = LabelEncoder()\n",
        "        df.label = le.fit_transform(df.label)\n",
        "        \n",
        "        # save\n",
        "        self.le = le\n",
        "        self.mura_frame = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mura_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.mura_frame.iloc[idx, 0])\n",
        "        \n",
        "        image = cv2.imread(img_name, 0)\n",
        "        ## thresholding\n",
        "        #image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)  \n",
        "        image = Image.fromarray(image)\n",
        "        image = image.convert('L')\n",
        "        image = self.transform(image)         \n",
        "        \n",
        "        label = self.mura_frame.iloc[idx, 1]\n",
        "        #label = torch.from_numpy(label.reshape(1, 1))\n",
        "        \n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8NFD14apuEJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainSet = MuraDataset(csv_file=train_all, root_dir=mura_prefix, \n",
        "                       transform=transforms.Compose([transforms.Resize((227,227)),\n",
        "                                                          transforms.ToTensor(),\n",
        "                                                          transforms.Normalize(\n",
        "                                                              mean=[0.456],\n",
        "                                                              std= [0.225])]))\n",
        "\n",
        "devSet = MuraDataset(csv_file=valid_all, root_dir=mura_prefix, \n",
        "                       transform=transforms.Compose([transforms.Resize((227,227)),\n",
        "                                                          transforms.ToTensor(),\n",
        "                                                          transforms.Normalize(\n",
        "                                                              mean=[0.456],\n",
        "                                                              std= [0.225])]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eEFw78Sruh7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "trainLoader = data.DataLoader(trainSet, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "devLoader = data.DataLoader(devSet, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "dataDict = {'train': trainLoader, 'valid': devLoader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oY-7lHrvulBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_alexnet():\n",
        "  alexnet = models.alexnet(pretrained=False)\n",
        "  for param in alexnet.parameters():\n",
        "    param.requires_grad = False\n",
        "  alexnet.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  alexnet.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=7, bias=True)\n",
        "  alexnet.to(device)  # transfer to gpu\n",
        "\n",
        "  optimizer = optim.Adam(alexnet.parameters(), lr=0.0013, weight_decay=1.0201e-06, amsgrad=True)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  return alexnet, optimizer, criterion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4a4wh9hiwFM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, \n",
        "                model_id, save_path = \"/content/models/\",\n",
        "                num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    train_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i, batch in enumerate(dataloaders[phase], 0):# inputs, labels in dataloaders[phase]:\n",
        "              #if i < 4:\n",
        "                inputs = batch['image'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                #running_accuracy = running_corrects.double() / ((i+1) * batch_size)\n",
        "                \n",
        "                #print('\\tbatch: {} Loss: {:.4f} Acc: {:.4f}'.format(i + 1, running_loss, running_accuracy))\n",
        "              \n",
        "              #else: break\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'valid' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            # append losses\n",
        "            if phase == 'valid':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            else:\n",
        "                train_acc_history.append(epoch_acc)\n",
        "                \n",
        "            # save model\n",
        "            save_model_path = save_path + model_id\n",
        "            if not os.path.exists(save_model_path):\n",
        "              os.makedirs(save_model_path)\n",
        "            torch.save(model.state_dict(), save_model_path + \"/epoch_\" + str(epoch))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, train_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7w-Hyxel4NYo",
        "colab_type": "code",
        "outputId": "45b76a31-509f-43aa-9a2b-458c603769b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1354
        }
      },
      "cell_type": "code",
      "source": [
        "alexnet, optimizer, criterion = init_alexnet()\n",
        "best, vh, th = train_model(alexnet, dataDict, criterion, optimizer, model_id = \"alex_scratch\", num_epochs=15)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 1.5014 Acc: 0.4375\n",
            "valid Loss: 1.3817 Acc: 0.4989\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 1.1309 Acc: 0.5971\n",
            "valid Loss: 1.1260 Acc: 0.6143\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.9716 Acc: 0.6601\n",
            "valid Loss: 0.9690 Acc: 0.6656\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.8752 Acc: 0.6963\n",
            "valid Loss: 0.8476 Acc: 0.7129\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.8015 Acc: 0.7222\n",
            "valid Loss: 0.8136 Acc: 0.7269\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.7535 Acc: 0.7363\n",
            "valid Loss: 0.7574 Acc: 0.7491\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.7130 Acc: 0.7519\n",
            "valid Loss: 0.7218 Acc: 0.7591\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.6835 Acc: 0.7634\n",
            "valid Loss: 0.7113 Acc: 0.7598\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.6698 Acc: 0.7701\n",
            "valid Loss: 0.6941 Acc: 0.7720\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.6362 Acc: 0.7808\n",
            "valid Loss: 0.6530 Acc: 0.7782\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.6231 Acc: 0.7855\n",
            "valid Loss: 0.6291 Acc: 0.7914\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.6042 Acc: 0.7911\n",
            "valid Loss: 0.6186 Acc: 0.7936\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.5780 Acc: 0.7994\n",
            "valid Loss: 0.6157 Acc: 0.7973\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.5766 Acc: 0.8010\n",
            "valid Loss: 0.6253 Acc: 0.7898\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.5668 Acc: 0.8056\n",
            "valid Loss: 0.5771 Acc: 0.8073\n",
            "\n",
            "Training complete in 38m 45s\n",
            "Best val Acc: 0.807319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dxCfD7-9b7F1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "alex net with thresholding -- best epoch 13 /content/models/alex_1"
      ]
    },
    {
      "metadata": {
        "id": "y4KOrOkB2bb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## copy models back to drive!!\n",
        "!cp -r /content/models/alex_scratch /content/gdrive/My\\ Drive/MURA_files/models/models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f90eCPVKDxCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "268499f0-feda-4ced-a90f-045bee29d3c1"
      },
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/My\\ Drive/MURA_files/models/models/"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alex_1\talex_nothreshold  alex_scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nxXEeajc-U8-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy_logs = pd.DataFrame({'epoch': np.arange(1, 16), 'train': [x.item() for x in th], 'valid': [x.item() for x in th]})\n",
        "accuracy_logs.to_csv('accuracy_logs.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3n_usdcJOVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp accuracy_logs.csv /content/gdrive/My\\ Drive/MURA_files/models/models/alex_scratch/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puPMYUHnJZVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}